{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json \n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm \n",
    "import pickle \n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from functions import *\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Renaming the columns dataset\n",
    "\"\"\"\n",
    "df = pd.read_csv(\"wikigraph_reduced.csv\", sep='\\t') \n",
    "df.columns = [\"Edges\",'Source', 'Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "save renamed dataset into a csv file\n",
    "\"\"\"\n",
    "df.to_csv('data/dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Taking as input the file wiki-topcats-page-names.txt, return a Dictionary with key the number of \n",
    "the page and as value the name of the page. \n",
    "\"\"\"\n",
    "# keys: number of the page (article)\n",
    "# values: name of the page (article)\n",
    "p = open(\"wiki-topcats-page-names.txt\", \"r\")\n",
    "pages = {}\n",
    "for pag in tqdm(p): \n",
    "    list_ = pag.split()[1:]\n",
    "    aux = ' '.join(list_)\n",
    "    pages[int(pag.split()[0])] = aux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pickle('data/pages.pkl', pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pages = read_pickle('data/pages.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Return the dictionary where as key the name of the category and \n",
    "as value the number of the articles that appears in the category. \n",
    "\"\"\"\n",
    "# keys: category\n",
    "# values: [list of the number of the pages (article)]\n",
    "cat = open(\"wiki-topcats-categories.txt\", \"r\")\n",
    "categories = {}\n",
    "for i in tqdm(cat): \n",
    "    category = i.split()[0][9:-1]\n",
    "    page_in_cat = list(map(int, i.split()[1:])) \n",
    "    categories[category] = page_in_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pickle('data/categories.pkl', categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = read_pickle('data/categories.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "each pages has a list of the names of the categories wich are linked to\n",
    "\"\"\"\n",
    "# keys: number of the page (article)\n",
    "# values: [list of categories]\n",
    "cat = open(\"wiki-topcats-categories.txt\", \"r\")\n",
    "cat_per_pages = {}\n",
    "for i in tqdm(cat):\n",
    "    category = i.split()[0][9:-1]\n",
    "    page_in_cat = list(map(int, i.split()[1:]))\n",
    "\n",
    "    if len(page_in_cat) >5000 and len(page_in_cat) < 30000:\n",
    "        for x in page_in_cat:\n",
    "            if x not in cat_per_pages:\n",
    "                a= []\n",
    "                a.append(category)\n",
    "                cat_per_pages[x]=a\n",
    "            else: \n",
    "                aux = list(cat_per_pages[x])\n",
    "                aux.append(category)\n",
    "                cat_per_pages[x]= aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "reducing cat_per_pages dictionary \n",
    "to have the condition that only one page is linked to only one category. \n",
    "\n",
    "\"\"\"\n",
    "# keys: number of the page (article)\n",
    "# values: category chosen at random and referring to that page\n",
    "one_cat_per_pages = {}\n",
    "for key, elem in tqdm(cat_per_pages.items()):\n",
    "    one_cat_per_pages[key] = random.choices(elem, k = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pickle('data/one_cat_per_pages.pkl', one_cat_per_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_cat_per_pages = read_pickle('data/one_cat_per_pages.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Finally, we create a last dictionary that takes us back to the initial state, i.e. \n",
    "the name of the category and the pages (integers) in that category. \n",
    "This time, each page is linked to a single category.\n",
    "\"\"\"\n",
    "# keys: category chosen at random \n",
    "# values: number of the pages (article) referring to the category\n",
    "categories_red = {}\n",
    "for key, elem in tqdm(one_cat_per_pages.items()):\n",
    "\n",
    "    if elem[0] not in categories_red:\n",
    "            a= []\n",
    "            a.append(key)\n",
    "            categories_red[elem[0]]=a\n",
    "    else: \n",
    "            aux = list(categories_red[elem[0]])\n",
    "            aux.append(key)\n",
    "            categories_red[elem[0]]= aux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pickle('data/categories_red.pkl', categories_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_red = read_pickle('data/categories_red.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = get_graph_dictionary(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pickle('data/graph.pkl', graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = read_pickle('data/graph.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the graph directed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TO DO: review- Kernel Stopped \n",
    "Initialize the Graph\n",
    "\"\"\"\n",
    "import networkx as nx\n",
    "import scipy as sp\n",
    "\n",
    "a = (g.edges())\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(g.edges())\n",
    "nx.is_directed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = nx.to_scipy_sparse_matrix(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsSymmetric(mat):\n",
    "    \"\"\"\n",
    "    Build a lil matrix to create a sparse matrix of the vertices and edges,\n",
    "    get the sum of the point in the matrix,\n",
    "    check if the matrix is symmetric or not\n",
    "    \"\"\"\n",
    "    mat = sp.lil_matrix((max(g.vertices())+1,max(g.vertices())+1), dtype=int)\n",
    "    # looping on each vertex to assign the edges == 1\n",
    "    for vertex in g.graph_d:   \n",
    "        for target in g.graph_d[vertex]:\n",
    "            mat[vertex, target] = 1\n",
    "    \n",
    "    rows, cols = mat.nonzero() # get only the non zero elements from the sparse matrix \n",
    "    return rows, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to check if the matrix is symmetric\n",
    "rows, cols = IsSymmetric(mat)\n",
    "if np.cumsum((mat[cols, rows] == mat[rows, cols]).A)[-1] == mat[cols, rows].shape[1]:\n",
    "    print('Is symmetric')\n",
    "else:\n",
    "    print('No symmetric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many articles are we considering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Aricles of the graph:\")\n",
    "print(len(g.vertices()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many hyperlinks between pages exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hyperlinks of the graph:\")\n",
    "print(len(g.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the average number of links in an arbitrary page. What is the graph density? Do you believe that the graph is dense or sparse? Is the graph dense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "prendo 100 pagine e calcolo la media del numero di link per pagina\n",
    "\"\"\"\n",
    "list_ = []\n",
    "for i in range(100):\n",
    "    list_.append(average_number_pages1(g))\n",
    "    avg = sum(list_)/len(list_)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the density of the graph. \n",
    "Since our graph is directed, we can compute the density as follows:\n",
    "    $$D = \\frac{\\lvert{E}\\rvert}{2\\binom{\\lvert{V}\\rvert}{2}} = \\frac{\\lvert{E}\\rvert}{\\lvert{V}\\rvert(\\lvert{V}\\rvert - 1)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_graph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, we may say that our graph is sparse because the density is close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the nodes' degree distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In degree Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = [data['Source'], data['Target']]\n",
    "df_concat = pd.concat(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = list(df_concat.unique())\n",
    "d_aux = dict.fromkeys(all_nodes, 0)\n",
    "only_target_node = list(data.Target)\n",
    "for node in tqdm(only_target_node):\n",
    "    d_aux[node] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_deg = Counter(sorted(list(d_aux.values())))\n",
    "y_in = np.array(list(in_deg.values()))\n",
    "y_in = y_in/len(g.vertices())\n",
    "x_in = list(in_deg.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,6))\n",
    "plt.bar(x_in , y_in,\n",
    "        color=(0.2, 0.4, 0.6, 0.6),\n",
    "        edgecolor='blue')\n",
    "plt.title(\"In Degree distribution of the firts 50 nodes \")\n",
    "plt.xlabel(\"In-degree\")\n",
    "plt.ylabel(\"Probability of node with in-degree = k \")\n",
    "plt.xlim(-1, 60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_deg_list = []\n",
    "for key,items in tqdm(g.graph_d.items()):\n",
    "    if isinstance(g.graph_d[key], list):\n",
    "         out_deg_list.append(len(g.graph_d[key]))\n",
    "    elif (isinstance(g.graph_d[key], int)): \n",
    "        out_deg_list.append(1) \n",
    "out_deg = Counter(sorted(out_deg_list))\n",
    "x_out = list(out_deg.keys())\n",
    "y_out =  np.array(list(out_deg.values()))/ len(g.vertices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ho troncato la distribuzione a 50 perchè andava troppo oltre e i valori erano tutti 0. \n",
    "\"\"\"\n",
    "plt.figure(figsize=(13,6))\n",
    "plt.bar(x_out, y_out,\n",
    "        color=(0.2, 0.4, 0.6, 0.6),\n",
    "        edgecolor='blue')\n",
    "plt.title(\"Out Degree distribution of the firts 50 nodes \")\n",
    "plt.xlabel(\"Out-degree\")\n",
    "plt.ylabel(\"Probability of node with out-degree = k \")\n",
    "plt.xlim(-1, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "dic = defaultdict(list)  # Creating default dictionary to store Source as key and Value as Target\n",
    "for key,value in zip(data['Source'],data['Target']): \n",
    "    dic[str(key)].append(str(value))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pages(page, click):\n",
    "    total_pages = [] # This list will store number of pages\n",
    "    page_list = [] #This list will store input vlaue initially and then will add correspondence value as per number of click\n",
    "    page_list.append(str(page))\n",
    "    for no_of_click in range(click): #This will run as per number of clicks\n",
    "        new_lst = []                 \n",
    "        for i in page_list:\n",
    "            for j in dic[i]:\n",
    "                new_lst.append(str(j))\n",
    "                total_pages.append(str(j))\n",
    "        page_list = new_lst\n",
    "    return total_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    page = input(\" Enter page number \")\n",
    "    click = input(\" Enter number of clicks \")\n",
    "    if page!='' or click!='':\n",
    "        total_pages = pages(page,int(click))\n",
    "        if len(total_pages)!=0:\n",
    "            print(\"User can reach {} pages after {} clicks\".format(len(total_pages),click))\n",
    "        else:\n",
    "            print(\"There is no link for this page, Kindly try with another page\")\n",
    "    else:\n",
    "        print(\" *********** Kindly provide valid input  **********\")\n",
    "except ValueError:\n",
    "    print(\"No valid input! Please try again ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that takes in input:\n",
    "- A category C\n",
    "- A set of pages in C, p = {p1, ..., pn}\n",
    "\n",
    "and returns the minimum number of clicks required to reach all pages in p, starting from the page v, corresponding to the most central article, according to the in-degree centrality, in C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum_number_clicks(graph, categories_red, data):\n",
    "    print('Write the category')\n",
    "    while True:\n",
    "        category_input = str(input())\n",
    "        if category_input not in categories_red:\n",
    "            print(category_input, ' not exist as category, change category')\n",
    "        else:\n",
    "            break\n",
    "    print()\n",
    "    print(\"Write the set of pages in the category chosen separated by a ','\")\n",
    "    print()\n",
    "    pages_input = input()\n",
    "    pages_input = pages_input.split(',')\n",
    "    pages_input = [int(i) for i in pages_input]\n",
    "\n",
    "    pages_not = []\n",
    "    for pages in pages_input:\n",
    "        if pages not in categories_red[category_input]:\n",
    "            print(pages, ' not in ', category_input)\n",
    "            pages_not.append(pages)\n",
    "    pages_input = [i for i in pages_input if i not in pages_not]  \n",
    "    \n",
    "    graph = g.graph_d                    # the graph\n",
    "    central_vertex = most_central_article(categories_red[category_input], in_degree_centrality(data))[0]   # set the max vertex\n",
    "    v = central_vertex\n",
    "    visited = [False] * (max(graph) + 1) # set as False the visited vertex\n",
    "    queue = []                           # set the queue list\n",
    "    queue.append(v)                      # append the starting vertex to the list\n",
    "    queue_complete = []                  # set the complete list to 0 \n",
    "    visited[v] = True                    # set the starting vertex as visited\n",
    "    reached = 0                          # initialize the number of reached vertex\n",
    "    reached_vertex = []                  # initialize the list of reached vertex\n",
    "    number_of_click = 0\n",
    "\n",
    "    while queue:\n",
    "        if reached < (len(pages_input)):\n",
    "            v = queue.pop(0)\n",
    "            #print(v, end=' ')\n",
    "\n",
    "            try:\n",
    "                number_of_click += 1\n",
    "                for i in graph[v]:\n",
    "                    if visited[i] == False:\n",
    "                        visited[i] = True\n",
    "                        queue.append(i)\n",
    "                        queue_complete.append(i)\n",
    "                        if i in pages_input:\n",
    "                            reached += 1\n",
    "                            reached_vertex.append(i)\n",
    "                            #print('Vertex', i, 'reached')\n",
    "            except TypeError:\n",
    "                number_of_click += 1\n",
    "                j = graph[v]\n",
    "                if visited[j] == False:\n",
    "                    visited[j] = True\n",
    "                    queue.append(j)\n",
    "                    queue_complete.append(j)\n",
    "                    if j in pages_input:\n",
    "                        reached += 1\n",
    "                        reached_vertex.append(j)\n",
    "                        #print('Vertex', i, 'reached')\n",
    "\n",
    "        else:\n",
    "            break\n",
    "    print('Reached vertex are: {}'.format(reached_vertex))\n",
    "    print('Minimum number of clicks, from most central article {} to reach the set of pages, is {}.'.format(central_vertex, number_of_click))\n",
    "    not_reached_vertex = [i for i in pages_input if i not in reached_vertex]\n",
    "    print('Not possible to reach {}'.format(not_reached_vertex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "minimum_number_clicks(g.graph_d, categories_red, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for el in categories_red['Living_people']:\n",
    "    if el in q:\n",
    "        print(el)\n",
    "    count+=1\n",
    "    if count == 10000:\n",
    "        print()\n",
    "        print('-'*20, count, '-'*20)\n",
    "        print()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question we were asked to implement a function that, given two categories as input, would return the subgraph induced by all the pages belonging to those two categories.\n",
    "\n",
    "Our reasoning in creating the function below was that the best course of action would be to consider all those pages that could be found in the \"source\" and \"target\" columns of the main dataset, that being ds. In this way, we could avoid selecting many of those pages found in these categories that have no links to any other page. So in the cat_subgraph function we firstly selected all the pages belonging to the two categories and then created a subset of the main dataset, taking into account only the pages belonging to edges that either connect the two categories or are within the categories themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_graph = cat_subgraph('Main_Belt_asteroids', 'Asteroids_named_for_people', ds)\n",
    "sub_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the structure of the subgraph is computed as a dictionary, we can apply the Graph class to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_g = Graph(sub_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can visualise the first order neighbours of a given node, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_edges_from(sub_g.edges())\n",
    "show_first_order_neigbors(G, start_node=865446)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### second function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second question of this request, we created two additional functions. The first one finds all the hyperlinks that connect the two nodes given as input, within the subgraph of their categories.\n",
    "\n",
    "It must be noted that, because of the preprocessing, not every pair of nodes has a meaningful output for this function. This is because many paths are missing in our dataframe, so that it is not possible to go from any node to any other node in the way we would in the complete Wikipedia. In fact, we observed that most of the nodes that have links are actually part of clusters of nodes, that is group of nodes that are connected to each other through paths. One such cluster is found as the output of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_hyperlinks(sub_graph, 865445, 865449)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function returns the amount of paths found, which comprises the minimum number of links that need to be cut in order to disconnect the two nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_hyperlinks(sub_graph, 865445, 865449)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the fifth question we had to write a function that would return the distance between a category given as input and every other category in our dataframe.\n",
    "\n",
    "As previously stated, because of the preprocessing and limitedness of the dataframe, many of the paths between nodes are missing, so that it is not possible to draw a complete path between any pair of nodes. For this reason, in the final output many distances are missing.\n",
    "\n",
    "# how do we handle these exceptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = [ds['Source'], ds['Target']]\n",
    "df_concat = pd.concat(concat)\n",
    "all_nodes = list(df_concat.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances_from_category(ic, cat, all_nodes):\n",
    "    results = {}\n",
    "\n",
    "    for c in tqdm(cat.keys()):\n",
    "        if c != ic:\n",
    "            visited = dict.fromkeys(all_nodes, False)\n",
    "            distance = dict.fromkeys(all_nodes, float('inf'))\n",
    "            sub_graph = cat_subgraph(c, ic, ds)\n",
    "            for key,val in sub_graph.items():\n",
    "                if isinstance(sub_graph[key], int):\n",
    "                    sub_graph[key] = [sub_graph[key]]\n",
    "            pages = relevant_pages(c, ds)\n",
    "            aux = []\n",
    "            for i in pages:\n",
    "                x = page_distance(i, ic, ds, sub_graph, visited, distance)\n",
    "                if x:\n",
    "                    aux.append(x)\n",
    "            merged = np.array(list(itertools.chain(*aux)))\n",
    "            m = np.median(merged)\n",
    "            results[c] = m\n",
    "            \n",
    "    write_pickle('data/' + ic, results)\n",
    "    return (results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the function consists of the remaining categories sorted in order of closeness to the category given as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_from_category('Main_Belt_asteroids', cat, all_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(categories_red['Buprestoidea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Target'] == 1185516].Source.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = [data['Source'], data['Target']]    # concat all the nodes\n",
    "all_nodes = list(pd.concat(concat).unique()) \n",
    "inbound = {}\n",
    "for node in all_nodes:\n",
    "    inbound[node] = data[data['Target'] == node].Source.values.tolist()\n",
    "#for _,row in data.iterrows():\n",
    "#    inbound[row['Target']].append(row['Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_vertex = dict.fromkeys(categories_red['Buprestoidea'], 1/len(categories_red['Buprestoidea'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inbound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 2\n",
    "for i in max_iterations:\n",
    "    normalized_vertex_temp = dict.fromkeys(categories_red['Buprestoidea'], 0) \n",
    "    for vertex in normalized_vertex:\n",
    "        pr = 0\n",
    "        for in_bound in inbound[vertex]:\n",
    "            pr += normalized_vertex[inbound]/out_degree_centrality(g)[inbound]\n",
    "        \n",
    "        normalized_vertex_temp[vertex] = pr\n",
    "    \n",
    "    normalized_vertex = normalized_vertex_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inbound[301]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_degree_centrality(g)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
